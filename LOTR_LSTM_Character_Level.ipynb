{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "gradient": {
     "editing": false,
     "id": "176b6d57-9c9b-4220-afc4-59cee518ee7f",
     "kernelId": ""
    },
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bengsoon/lstm_lord_of_the_rings/blob/main/LOTR_LSTM_Character_Level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "78a50d87-f14f-458e-a5cc-b253e4980a76",
     "kernelId": ""
    },
    "id": "sBQN-dtIGzPk"
   },
   "source": [
    "## Creating a Language Model with LSTM using Lord of The Rings Corpus\n",
    "In this notebook, we will create a character-level language language model using LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "59f1b821-f91c-49d6-b16b-f197ecbf79be",
     "kernelId": ""
    },
    "id": "BwvwLm9wnCwc"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 42,
     "id": "f684ee60-982f-4b75-86c1-f693b9be6c0f",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "## for paperspace \n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 38,
     "id": "773c737a-b01a-4657-88cb-c37e1027ada4",
     "kernelId": ""
    },
    "id": "o75lMt9gIoRf"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding, Input, LSTM, Flatten, Dense, Dropout\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np \n",
    "\n",
    "from pprint import pprint as pp\n",
    "from string import punctuation\n",
    "import regex as re\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "05b0a9be-f651-4ef1-967d-3bdcc1fe0d92",
     "kernelId": ""
    },
    "id": "sMsUyeLHnFZC"
   },
   "source": [
    "### Data Preprocessing & Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 2,
     "id": "1386aa19-e543-4fac-b7ea-edb3749341c4",
     "kernelId": ""
    },
    "id": "t4_puY1vH-Jv"
   },
   "outputs": [],
   "source": [
    "# get LOTR full text\n",
    "# !wget https://raw.githubusercontent.com/bengsoon/lstm_lord_of_the_rings/main/lotr_full.txt -P /content/drive/MyDrive/Colab\\ Notebooks/LOTR_LSTM/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "83cd9851-5afc-4ebf-ae8d-944baf3678bb",
     "kernelId": ""
    },
    "id": "s065iL9iBq_A"
   },
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 6,
     "id": "8fd79a20-9a83-4fd7-8b0d-371e83c382e9",
     "kernelId": ""
    },
    "id": "SIYll8_1BycF"
   },
   "outputs": [],
   "source": [
    "path = Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": false,
     "execution_count": 7,
     "id": "e55354c5-aacc-456c-8a85-9db542b7e560",
     "kernelId": ""
    },
    "id": "8IVk7GxXn40r",
    "outputId": "7cffa039-02bb-4d40-ddec-92a0f2cff911"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three Rings for the Elven-kings under the sky,\n",
      "               Seven for the Dwarf-lords in their halls of stone,\n",
      "            Nine for Mortal Men doomed to die,\n",
      "              One for the Dark Lord on his dark throne\n",
      "           In the Land of Mordor where the Shadows lie.\n",
      "               One Ring to rule them all, One Ring to find them,\n",
      "               One Ring to bring them all and in the darkness bind them\n",
      "           In the Land of Mordor where the Shadows lie.\n",
      "           \n",
      "FOREWORD\n",
      "\n",
      "This tale grew in the telling, until it became a history of the Great War of the Ring and included many glimpses of the yet more ancient history that preceded it. It was begun soon after _The Hobbit_ was written and before its publication in 1937; but I did not go on with this sequel, for I wished first to complete and set in order the mythology and legends of the Elder Days, which had then been taking shape for some years. I desired to do this for my own satisfaction, and I had little hope that other people \n"
     ]
    }
   ],
   "source": [
    "with open(path / \"data/lotr_full.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": false,
     "execution_count": 8,
     "id": "1069fe23-72ce-4f89-81ef-3aaaf082df18",
     "kernelId": ""
    },
    "id": "RjEFSBDEofqz",
    "outputId": "7b492d33-fb6e-4b3f-870f-9c9b025f7516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 1532.723 K characters\n"
     ]
    }
   ],
   "source": [
    "print(f\"Corpus length: {int(len(text)) / 1000 } K characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "20c107c1-53fa-4599-ac7c-a6c07169ad6e",
     "kernelId": ""
    },
    "id": "uhpey-V0ooM7"
   },
   "source": [
    "#### Unique Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 9,
     "id": "41392923-c71c-41c9-9e5c-08c2f68f926e",
     "kernelId": ""
    },
    "id": "wCcNzpC8o0rD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique characters: 93\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(list(text)))\n",
    "print(\"Total unique characters: %s\" % (len(chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 10,
     "id": "8519e226-ce2e-46f0-a964-8c3f0dccd5c3",
     "kernelId": ""
    },
    "id": "Lfip0JCRfrMT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'É', 'Ó', 'á', 'â', 'ä', 'é', 'ë', 'í', 'ó', 'ú', 'û', '–']\n"
     ]
    }
   ],
   "source": [
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFvk_sHgpEbq"
   },
   "source": [
    "#### Preparing X & y Datasets\n",
    "\n",
    "We need to split the text into two sets of fixed-size character sequences (X & y)\n",
    "* The first sequence (`sentences`) is the input data where the model will receive a fixed-size (`MAX_SEQ_LEN`) character sequence\n",
    "* The second sequence (`next_chars`) is the output data, which is only 1 character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 11,
     "id": "f7b0e649-e019-4b5d-918d-82d69eefd45e",
     "kernelId": ""
    },
    "id": "kzS0BUamA8_f"
   },
   "outputs": [],
   "source": [
    "# setting up model constants\n",
    "MAX_SEQ_LEN = 20\n",
    "MAX_FEATURES = len(chars)\n",
    "step = 2\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "execution_count": 12,
     "id": "bbbded66-88b9-472d-a095-497a6d4678b5",
     "kernelId": ""
    },
    "id": "kfx1E_2-DNi4",
    "outputId": "d0fd4bf2-9d8b-472c-9d93-a8b2d63be711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training examples: 766352\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - MAX_SEQ_LEN, step):\n",
    "    sentences.append(text[i: i + MAX_SEQ_LEN])\n",
    "    next_chars.append(text[i + MAX_SEQ_LEN])\n",
    "\n",
    "print(\"Total number of training examples:\", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "execution_count": 13,
     "id": "4962cae2-1362-4270-bda6-822be01d47d7",
     "kernelId": ""
    },
    "id": "1nm8ok_eFKp4",
    "outputId": "880af11d-8fd2-4303-c866-bf2d7d28f73c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r: its springs were  ..... a\n",
      " me.'\n",
      "     Pippin la ..... u\n",
      "ite face, her hand c ..... l\n",
      " on the open hill, b ..... e\n",
      " birthday, which he  ..... c\n",
      "thers must take refu ..... g\n",
      "bbits referred to th ..... o\n",
      "fed me, and so I'm b ..... e\n",
      " there, silent and a ..... l\n",
      " as his fortune allo ..... w\n"
     ]
    }
   ],
   "source": [
    "# randomly sample some of the input and output to visualize\n",
    "for i in range(10):\n",
    "    ix = random.randint(0, len(sentences))\n",
    "    print(f\"{sentences[ix]} ..... {next_chars[ix]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 14,
     "id": "b3d807b3-5e78-4f8d-bda3-ad9c20874c9e",
     "kernelId": ""
    },
    "id": "NttXyeNkmxBQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 05:33:56.365908: W tensorflow/stream_executor/platform/default/dso_loader.cc:65] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-11-19 05:33:56.366495: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-19 05:33:56.366861: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (n0oqzbkbv5): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "X_train_raw = tf.data.Dataset.from_tensor_slices(sentences)\n",
    "y_train_raw = tf.data.Dataset.from_tensor_slices(next_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "execution_count": 15,
     "id": "a662c150-4346-4d82-9c66-54cb4d753401",
     "kernelId": ""
    },
    "id": "nS6ZY_vQnur9",
    "outputId": "32640daf-e86e-42a2-9f0e-243111bfc662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three Rings for the  ..... E\n",
      "ree Rings for the El ..... v\n",
      "e Rings for the Elve ..... n\n",
      "Rings for the Elven- ..... k\n",
      "ngs for the Elven-ki ..... n\n"
     ]
    }
   ],
   "source": [
    "for input, output in zip(X_train_raw.take(5), y_train_raw.take(5)):\n",
    "    print(f\"{input.numpy().decode('utf-8')} ..... {output.numpy().decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-RoO5g9oF11"
   },
   "source": [
    "#### Preprocessing with Keras `TextVectorization` layer\n",
    "[_doc_](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization)\n",
    "\n",
    "We will use the `TextVectorization` layer as the preprocessing pipeline for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 16,
     "id": "e44f62e7-fe35-4fac-aec0-e02e5ba7acc6",
     "kernelId": ""
    },
    "id": "CPkk5KYfpOGG"
   },
   "outputs": [],
   "source": [
    "def standardize_text(input):\n",
    "    \"\"\"\n",
    "        create a custom standardization that:\n",
    "            1. Fixes whitespaces \n",
    "            2. Removes punctuations & numbers\n",
    "            3. Sets all texts to lowercase\n",
    "            4. Preserves the Elvish characters\n",
    "    \"\"\"\n",
    "    \n",
    "    input = tf.strings.regex_replace(input, r\"[\\s+]\", \" \")\n",
    "    input = tf.strings.regex_replace(input, r\"[0-9]\", \"\")\n",
    "    input = tf.strings.regex_replace(input, f\"[{punctuation}–]\", \"\")\n",
    "\n",
    "    return tf.strings.lower(input)\n",
    "\n",
    "def char_split(input):\n",
    "    return tf.strings.unicode_split(input, 'UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 17,
     "id": "a0e4373a-17f2-4e9a-9776-9516d663178b",
     "kernelId": ""
    },
    "id": "2YsTfuGvr1Yz"
   },
   "outputs": [],
   "source": [
    "# create text vectorization layer\n",
    "vectorization_layer = TextVectorization(\n",
    "    max_tokens = MAX_FEATURES,\n",
    "    standardize = standardize_text,\n",
    "    split = char_split,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQ_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 18,
     "id": "29cd6936-f807-453e-9fd9-6af8078b608d",
     "kernelId": ""
    },
    "id": "Av2-nTMwVg3i"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 05:34:06.910154: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "# create the vocabulary indexing with `adapt`\n",
    "vectorization_layer.adapt(X_train_raw.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "execution_count": 19,
     "id": "fb3a892c-8cc8-4e89-9503-07020bf75395",
     "kernelId": ""
    },
    "id": "Ha0e8zdNZM9A",
    "outputId": "e1422f91-7b65-4537-9c8f-72e082582fc6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique characters: 40\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total unique characters: {len(vectorization_layer.get_vocabulary())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "execution_count": 20,
     "id": "4973f32d-09ce-46d4-92f6-4448f7305677",
     "kernelId": ""
    },
    "id": "VfACnSAHcLo7",
    "outputId": "e606410c-9f70-44a0-e618-e40cf6d3fc90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', ' ', 'e', 't', 'a', 'o', 'n', 'h', 'i', 's', 'r', 'd', 'l', 'w', 'u', 'f', 'g', 'm', 'y', 'b', 'c', 'p', 'k', 'v', 'j', 'q', 'x', 'z', 'ó', 'É', 'ú', 'û', 'é', 'á', 'í', 'ë', 'â', 'ä', 'Ó']\n"
     ]
    }
   ],
   "source": [
    "print(vectorization_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 21,
     "id": "bcd25c53-7c94-4615-88b6-765e7c60b4f5",
     "kernelId": ""
    },
    "id": "UlqZ_iM2cQGT"
   },
   "outputs": [],
   "source": [
    "def vectorize_text(text):\n",
    "    \"\"\" Convert text into a Tensor using vectorization_layer\"\"\"\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return tf.squeeze(vectorization_layer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "execution_count": 22,
     "id": "be28aa20-e0ae-4268-a8a2-29c8f82837f3",
     "kernelId": ""
    },
    "id": "FNWzpoeyc9q9",
    "outputId": "24674c2c-ef3c-4566-9b1b-658d6d5d7da6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=int64, numpy=\n",
       "array([ 8,  3, 13, 13,  6,  2,  9,  2,  5, 18,  2,  8,  6,  5,  8,  5,  0,\n",
       "        0,  0,  0])>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = \"hello i am Hoaha\"\n",
    "\n",
    "vectorize_text(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbCiQhJpd5F-"
   },
   "source": [
    "#### Apply Text Vectorization to X & y datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "execution_count": 23,
     "id": "1b4eb931-92d0-4b7c-801c-6fe8bcf3ae8e",
     "kernelId": ""
    },
    "id": "_yG2zJ-LdE8U",
    "outputId": "86a50c67-2810-4f86-c468-d343dd224c28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(20,), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(20,), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize the dataset\n",
    "X_train = X_train_raw.map(vectorize_text)\n",
    "y_train = y_train_raw.map(vectorize_text)\n",
    "\n",
    "X_train.element_spec, y_train.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "execution_count": 24,
     "id": "e76873e0-5e5e-4a34-98dd-515ca639ed55",
     "kernelId": ""
    },
    "id": "g9Pj4qNKedwa",
    "outputId": "83d92ee7-0f57-408c-e319-92ee1224e315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(20,), dtype=int64)\n",
      "tf.Tensor([24  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0], shape=(20,), dtype=int64)\n",
      "tf.Tensor([7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(20,), dtype=int64)\n",
      "tf.Tensor([23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0], shape=(20,), dtype=int64)\n",
      "tf.Tensor([7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(20,), dtype=int64)\n",
      "tf.Tensor([10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0], shape=(20,), dtype=int64)\n",
      "tf.Tensor([15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0], shape=(20,), dtype=int64)\n",
      "tf.Tensor([12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0], shape=(20,), dtype=int64)\n",
      "tf.Tensor([11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0], shape=(20,), dtype=int64)\n",
      "tf.Tensor([4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(20,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for elem in y_train.take(10):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 25,
     "id": "0d1be1a9-0bb8-4888-91da-ae9d13c25dd6",
     "kernelId": ""
    },
    "id": "UnWusGJVeuhK"
   },
   "outputs": [],
   "source": [
    "# we only one the first representation in the vector in the y_train dataset\n",
    "y_train = y_train.map(lambda y: y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "execution_count": 26,
     "id": "735701a3-6097-4741-b15c-f8567755af70",
     "kernelId": ""
    },
    "id": "sJEa6PQvfbEU",
    "outputId": "47fcb6ed-ee19-42b6-c87b-c5350d207105"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: ()\n",
      "Next Character: 3\n",
      "Shape: ()\n",
      "Next Character: 24\n",
      "Shape: ()\n",
      "Next Character: 7\n",
      "Shape: ()\n",
      "Next Character: 23\n",
      "Shape: ()\n",
      "Next Character: 7\n"
     ]
    }
   ],
   "source": [
    "for elem in y_train.take(5):\n",
    "    print(f\"Shape: {elem.shape}\")\n",
    "    print(f\"Next Character: {elem.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "execution_count": 27,
     "id": "48c6be67-452b-4d38-9826-c57e7391c344",
     "kernelId": ""
    },
    "id": "eBFi_L2lgL2R",
    "outputId": "f995db00-6073-484e-ce8a-f9b8f667c745"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<TakeDataset shapes: (20,), types: tf.int64>,\n",
       " <TakeDataset shapes: (), types: tf.int64>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check tensor dimensions to ensure we have MAX_SEQ_LEN-sized inputs and single output\n",
    "X_train.take(1), y_train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "execution_count": 28,
     "id": "d9adef0f-856f-4fad-8ec2-6743d63b183a",
     "kernelId": ""
    },
    "id": "1Q7FdsXOg2XQ",
    "outputId": "24ea6572-7ff8-42f6-ee8b-6aca79ccd764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4  8 11  3  3  2 11  9  7 17 10  2 16  6 11  2  4  8  3  2] ------------>  3\n",
      "[11  3  3  2 11  9  7 17 10  2 16  6 11  2  4  8  3  2  3 13] ------------>  24\n",
      "[ 3  2 11  9  7 17 10  2 16  6 11  2  4  8  3  2  3 13 24  3] ------------>  7\n",
      "[11  9  7 17 10  2 16  6 11  2  4  8  3  2  3 13 24  3  7  0] ------------>  23\n",
      "[ 7 17 10  2 16  6 11  2  4  8  3  2  3 13 24  3  7 23  9  0] ------------>  7\n"
     ]
    }
   ],
   "source": [
    "for input, output in zip(X_train.take(5), y_train.take(5)):\n",
    "    print(f\"{input.numpy()} ------------>  {output.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8p3YiJRg_hP"
   },
   "source": [
    "#### Bringing the data pipeline together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRbr9Ub1hJ8K"
   },
   "source": [
    "**Joining the X and y into a dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 29,
     "id": "3a46059d-5ee2-4c74-9fad-c51af7313b15",
     "kernelId": ""
    },
    "id": "M6G8kWIphMw-"
   },
   "outputs": [],
   "source": [
    "# joining X & y into a single dataset\n",
    "train_dataset = tf.data.Dataset.zip((X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6N8Q5VeghVSa"
   },
   "source": [
    "**Setting data pipeline optimizations:**\n",
    "Perform async prefetching / buffering of data using AUTOTUNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 30,
     "id": "4e44406e-1796-497e-855d-d0b3e7268d23",
     "kernelId": ""
    },
    "id": "PY_1f_H0hfXH"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.prefetch(buffer_size=512).batch(BATCH_SIZE, drop_remainder=True).cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "execution_count": 31,
     "id": "bba515af-3a59-42f4-be39-3bd6848408da",
     "kernelId": ""
    },
    "id": "06t7wVLEhvBY",
    "outputId": "28b7317f-ecca-4ea2-975d-32c59f4abbf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset in batches: 11974\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of the dataset in batches: {train_dataset.cardinality().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "execution_count": 32,
     "id": "c35d095c-46a9-404d-ad45-a2f96a53e97f",
     "kernelId": ""
    },
    "id": "J3LTZqwKh3Eg",
    "outputId": "a94c1d64-6027-4bb5-fd5f-8100f0bad166"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (X) Dimension: (64, 20)\n",
      "Output (y) Dimension: (64,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 05:35:15.633059: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# check the tensor dimensions of X and y again\n",
    "\n",
    "for sample in train_dataset.take(1):\n",
    "    print(f\"Input (X) Dimension: {sample[0].numpy().shape}\")\n",
    "    print(f\"Output (y) Dimension: {sample[1].numpy().shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCDje6OdiOBv"
   },
   "source": [
    "### Build the LSTM Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 33,
     "id": "7ef0a756-2fa1-47d8-9a4f-418293cc9af6",
     "kernelId": ""
    },
    "id": "ff-C3pnnlR1D"
   },
   "outputs": [],
   "source": [
    "def char_LSTM_model(max_seq_len=MAX_SEQ_LEN, max_features=MAX_FEATURES, embedding_dim=EMBEDDING_DIM):\n",
    "\n",
    "    # Define input for the model (vocab indices)\n",
    "    inputs = tf.keras.Input(shape=(max_seq_len), dtype=\"int64\")\n",
    "\n",
    "    # Add a layer to map the vocab indices into an embedding layer\n",
    "    X = Embedding(max_features, embedding_dim)(inputs)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(128, return_sequences=True)(X)\n",
    "    X = Flatten()(X)\n",
    "    outputs = Dense(max_features, activation=\"softmax\")(X)\n",
    "    model = Model(inputs, outputs, name=\"model_LSTM\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "execution_count": 34,
     "id": "685f775c-a5f3-4c30-a5f9-b7e5eec55cba",
     "kernelId": ""
    },
    "id": "gpI324yXrCCy",
    "outputId": "8ceaac43-e15e-4211-9efa-4ba6955078b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_LSTM\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 20, 16)            1488      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 20, 16)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 20, 128)           74240     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 93)                238173    \n",
      "=================================================================\n",
      "Total params: 313,901\n",
      "Trainable params: 313,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = char_LSTM_model()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 35,
     "id": "75f802be-3d46-48df-9872-ef2e635ebd01",
     "kernelId": ""
    },
    "id": "Ing-AtSxllnH"
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=0.2):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds=np.squeeze(preds)\n",
    "    \n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "    \n",
    "def generate_text(model, seed_original, step, diversity):\n",
    "    seed=vectorize_text(seed_original)\n",
    "    # decode_sentence(seed.numpy().squeeze())\n",
    "    print(f\"Starting the sentence with.... '{seed_original}'\")\n",
    "    print(\"...Diversity:\", diversity)\n",
    "    seed= vectorize_text(seed_original).numpy().reshape(1,-1)\n",
    "    \n",
    "    generated = (seed)\n",
    "    for i in range(step):\n",
    "        predictions=model.predict(seed)\n",
    "        pred_max= np.argmax(predictions.squeeze())\n",
    "        next_index = sample(predictions, diversity)\n",
    "        generated = np.append(generated, next_index)\n",
    "        seed= generated[-MAX_SEQ_LEN:].reshape(1,MAX_SEQ_LEN)\n",
    "    return decode_sentence(generated)\n",
    "\n",
    "\n",
    "def decode_sentence (encoded_sentence):\n",
    "    deceoded_sentence=[]\n",
    "    for word in encoded_sentence:\n",
    "        deceoded_sentence.append(vectorization_layer.get_vocabulary()[word])\n",
    "    sentence= ''.join(deceoded_sentence)\n",
    "    print(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "id": "aa6f8b8e-0ace-4258-86ed-ec0be2aecead",
     "kernelId": ""
    },
    "id": "zSj2zhTQrTZe"
   },
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights\n",
    "checkpoint_path = path / \"models/model_cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 save_weights_only=True, \n",
    "                                                 verbose=1)\n",
    "\n",
    "# Train the model\n",
    "epochs = 30\n",
    "BATCH_SIZE = 64\n",
    "SAMPLING_STEPS = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"-\"*40 + f\"  Epoch: {epoch}/{epochs}  \" + \"-\"*40)\n",
    "    model.fit(train_dataset, batch_size=BATCH_SIZE, epochs=1, callbacks=[cp_callback])\n",
    "    print()\n",
    "    print(\"*\"*30 + f\" Generating text after epoch #{epoch} \" + \"*\"*30)\n",
    "    start_index = random.randint(0, len(text) - MAX_SEQ_LEN - 1)\n",
    "    sentence = text[start_index : start_index + MAX_SEQ_LEN]\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        generate_text(model, sentence, SAMPLING_STEPS, diversity)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "id": "e3b76f5c-262e-43c8-a92b-4dcabd56c006",
     "kernelId": ""
    },
    "id": "CXRI53ovotoS"
   },
   "outputs": [],
   "source": [
    "model.save(path / \"models/Char_LSTM_LOTR_20211112-1.h5\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 40,
     "id": "9cbb9669-e6d4-44a3-ae79-88a8f3759e1a",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "model = load_model(path / \"models/Char_LSTM_LOTR_20211112-1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "execution_count": 43,
     "id": "c35c4fe0-44bf-4d5c-9859-47cf5140c2bb",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11974/11974 [==============================] - 1171s 98ms/step - loss: 1.3499 - accuracy: 0.5751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3499177694320679, 0.5750871896743774]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train_dataset, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "id": "b4bfe53d-fb56-4b9a-8ccf-a1fb96adb0d2",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMUVcahO4a3DXhcS3qZYvnT",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1Nm_Lx9Eq95SA74PfH_qDUDVUJUsLVnMB",
   "name": "LOTR_LSTM_Character_Level.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
