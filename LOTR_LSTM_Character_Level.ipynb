{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LOTR_LSTM_Character_Level.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Nm_Lx9Eq95SA74PfH_qDUDVUJUsLVnMB",
      "authorship_tag": "ABX9TyPFAxKpjaik5/tFjFoEyKF8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bengsoon/lstm_lord_of_the_rings/blob/main/LOTR_LSTM_Character_Level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBQN-dtIGzPk"
      },
      "source": [
        "## Creating a Language Model with LSTM using Lord of The Rings Corpus\n",
        "In this notebook, we will create a character-level language language model using LSTM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwvwLm9wnCwc"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qomp39P1t4Ih"
      },
      "source": [
        "# set up GitHub API \n",
        "# !pip install ghapi\n",
        "from ghapi.all import GhApi\n",
        "!source /content/drive/MyDrive/Colab\\ Notebooks/git_auth.sh\n",
        "\n",
        "gh_api = GhApi(owner=\"bengsoon\", repo=\"lstm_lord_of_the_rings\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "BBR9-HPC2I2X",
        "outputId": "4a2f3350-53ac-431e-adf5-ec92e6ca8fed"
      },
      "source": [
        "gh_api.git.get_ref(owner=\"bengsoon\", repo=\"lstm_lord_of_the_rings\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTP404NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTP404NotFoundError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7b9eefae9372>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgh_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bengsoon\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lstm_lord_of_the_rings\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ghapi/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, headers, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m         route_p,query_p,data_p = [{p:kwargs[p] for p in o if p in kwargs}\n\u001b[1;32m     62\u001b[0m                                  for o in (self.route_ps,self.params,d)]\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroute_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34mf'{self.tag}.{self.name}{signature(self)}\\n{self.doc_url}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ghapi/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, path, verb, headers, route, query, data)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mroute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroute\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroute\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         res,self.recv_hdrs = urlsend(path, verb, headers=headers or None, debug=self.debug, return_headers=True,\n\u001b[0;32m--> 109\u001b[0;31m                                      route=route or None, query=query or None, data=data or None)\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'X-RateLimit-Remaining'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_hdrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mnewlim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_hdrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X-RateLimit-Remaining'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastcore/net.py\u001b[0m in \u001b[0;36murlsend\u001b[0;34m(url, verb, headers, route, query, data, json_data, return_json, return_headers, debug)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0murlread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_headers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murlread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_headers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastcore/net.py\u001b[0m in \u001b[0;36murlread\u001b[0;34m(url, data, headers, decode, return_json, return_headers, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhdrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mExceptionsHTTP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTP404NotFoundError\u001b[0m: HTTP Error 404: Not Found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o75lMt9gIoRf"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras.layers import Embedding, Input, LSTM, Flatten, Dense, Dropout\n",
        "from tensorflow.keras import Model\n",
        "import numpy as np \n",
        "\n",
        "from pprint import pprint as pp\n",
        "from string import punctuation\n",
        "import regex as re\n",
        "import random\n",
        "import os\n",
        "from ghapi.all import GhApi"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMsUyeLHnFZC"
      },
      "source": [
        "### Data Preprocessing & Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4_puY1vH-Jv"
      },
      "source": [
        "# get LOTR full text\n",
        "# !wget https://raw.githubusercontent.com/bengsoon/lstm_lord_of_the_rings/main/lotr_full.txt -P /content/drive/MyDrive/Colab\\ Notebooks/LOTR_LSTM/data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s065iL9iBq_A"
      },
      "source": [
        "#### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIYll8_1BycF"
      },
      "source": [
        "path = r\"/content/drive/MyDrive/Colab Notebooks/LOTR_LSTM/data/lotr_full.txt\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IVk7GxXn40r",
        "outputId": "7cffa039-02bb-4d40-ddec-92a0f2cff911"
      },
      "source": [
        "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "print(text[:1000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Three Rings for the Elven-kings under the sky,\n",
            "               Seven for the Dwarf-lords in their halls of stone,\n",
            "            Nine for Mortal Men doomed to die,\n",
            "              One for the Dark Lord on his dark throne\n",
            "           In the Land of Mordor where the Shadows lie.\n",
            "               One Ring to rule them all, One Ring to find them,\n",
            "               One Ring to bring them all and in the darkness bind them\n",
            "           In the Land of Mordor where the Shadows lie.\n",
            "           \n",
            "FOREWORD\n",
            "\n",
            "This tale grew in the telling, until it became a history of the Great War of the Ring and included many glimpses of the yet more ancient history that preceded it. It was begun soon after _The Hobbit_ was written and before its publication in 1937; but I did not go on with this sequel, for I wished first to complete and set in order the mythology and legends of the Elder Days, which had then been taking shape for some years. I desired to do this for my own satisfaction, and I had little hope that other people \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjEFSBDEofqz",
        "outputId": "7b492d33-fb6e-4b3f-870f-9c9b025f7516"
      },
      "source": [
        "print(f\"Corpus length: {int(len(text)) / 1000 } K characters\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus length: 1532.723 K characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhpey-V0ooM7"
      },
      "source": [
        "#### Unique Characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCcNzpC8o0rD"
      },
      "source": [
        "# chars = sorted(set(list(text)))\n",
        "# print(\"Total unique characters: %s\" % (len(chars)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfip0JCRfrMT"
      },
      "source": [
        "# print(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLc83JxugdZE"
      },
      "source": [
        "Let's remove all the whitespaces like `\\t`, `\\n` and ` `, as well as all the other `punctuation`, numbers and `\\–`. We want to keep the other Elvish characters as well as the capitalizations to preserve names etc "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n-p5xsPhS6T"
      },
      "source": [
        "# punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-9Lv1cOg7Vh"
      },
      "source": [
        "# text = re.sub(r\"[\\s+]\", \" \", text)\n",
        "# text = re.sub(f\"[0-9\\–{punctuation}]\", \"\", text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMOFdHItiGQc",
        "outputId": "71ee6a70-541f-41ad-8d03-51957fb72402"
      },
      "source": [
        "chars = sorted(set(list(text)))\n",
        "print(\"Total unique characters: %s\" % (len(chars)))\n",
        "print(chars)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique characters: 93\n",
            "['\\t', '\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'É', 'Ó', 'á', 'â', 'ä', 'é', 'ë', 'í', 'ó', 'ú', 'û', '–']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9svJcFemkMSR"
      },
      "source": [
        "#### Create dictionary to convert letters to integers and vice versa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wyIxnHjASQ-"
      },
      "source": [
        "# char2idx = {c: i for i, c in enumerate(chars)}\n",
        "# idx2char = {i: c for c, i in char2idx.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXzOQqNfAjSd"
      },
      "source": [
        "# print(char2idx)\n",
        "# print(\"*\" * 20)\n",
        "# print(idx2char)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFvk_sHgpEbq"
      },
      "source": [
        "#### Preparing X & y Datasets\n",
        "\n",
        "We need to split the text into two sets of fixed-size character sequences (X & y)\n",
        "* The first sequence (`sentences`) is the input data where the model will receive a fixed-size (`MAX_SEQ_LEN`) character sequence\n",
        "* The second sequence (`next_chars`) is the output data, which is only 1 character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzS0BUamA8_f"
      },
      "source": [
        "# setting up model constants\n",
        "MAX_SEQ_LEN = 20\n",
        "MAX_FEATURES = len(chars)\n",
        "step = 2\n",
        "BATCH_SIZE = 64\n",
        "EMBEDDING_DIM = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfx1E_2-DNi4",
        "outputId": "d0fd4bf2-9d8b-472c-9d93-a8b2d63be711"
      },
      "source": [
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0, len(text) - MAX_SEQ_LEN, step):\n",
        "    sentences.append(text[i: i + MAX_SEQ_LEN])\n",
        "    next_chars.append(text[i + MAX_SEQ_LEN])\n",
        "\n",
        "print(\"Total number of training examples:\", len(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of training examples: 766352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nm8ok_eFKp4",
        "outputId": "880af11d-8fd2-4303-c866-bf2d7d28f73c"
      },
      "source": [
        "# randomly sample some of the input and output to visualize\n",
        "for i in range(10):\n",
        "    ix = random.randint(0, len(sentences))\n",
        "    print(f\"{sentences[ix]} ..... {next_chars[ix]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " me all the way to t ..... h\n",
            "oon.'\n",
            "     'Noon?' s ..... a\n",
            "any king of the Mark .....  \n",
            " he still lived in t ..... h\n",
            "e hasn't taken the v ..... i\n",
            "ry crawled on all fo ..... u\n",
            "ts of the King grew  ..... a\n",
            "on of Arathorn, chie ..... f\n",
            ", some thirty feel b ..... e\n",
            "he had a good deal o ..... f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NttXyeNkmxBQ"
      },
      "source": [
        "X_train_raw = tf.data.Dataset.from_tensor_slices(sentences)\n",
        "y_train_raw = tf.data.Dataset.from_tensor_slices(next_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS6ZY_vQnur9",
        "outputId": "32640daf-e86e-42a2-9f0e-243111bfc662"
      },
      "source": [
        "for input, output in zip(X_train_raw.take(5), y_train_raw.take(5)):\n",
        "    print(f\"{input.numpy().decode('utf-8')} ..... {output.numpy().decode('utf-8')}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Three Rings for the  ..... E\n",
            "ree Rings for the El ..... v\n",
            "e Rings for the Elve ..... n\n",
            "Rings for the Elven- ..... k\n",
            "ngs for the Elven-ki ..... n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-RoO5g9oF11"
      },
      "source": [
        "#### Preprocessing with Keras `TextVectorization` layer\n",
        "[_doc_](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization)\n",
        "\n",
        "We will use the `TextVectorization` layer as the preprocessing pipeline for our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPkk5KYfpOGG"
      },
      "source": [
        "def standardize_text(input):\n",
        "    \"\"\"\n",
        "        create a custom standardization that:\n",
        "            1. Fixes whitespaces \n",
        "            2. Removes punctuations & numbers\n",
        "            3. Sets all texts to lowercase\n",
        "            4. Preserves the Elvish characters\n",
        "    \"\"\"\n",
        "    \n",
        "    input = tf.strings.regex_replace(input, r\"[\\s+]\", \" \")\n",
        "    input = tf.strings.regex_replace(input, r\"[0-9]\", \"\")\n",
        "    input = tf.strings.regex_replace(input, f\"[{punctuation}–]\", \"\")\n",
        "\n",
        "    return tf.strings.lower(input)\n",
        "\n",
        "def char_split(input):\n",
        "    return tf.strings.unicode_split(input, 'UTF-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YsTfuGvr1Yz"
      },
      "source": [
        "# create text vectorization layer\n",
        "vectorization_layer = TextVectorization(\n",
        "    max_tokens = MAX_FEATURES,\n",
        "    standardize = standardize_text,\n",
        "    split = char_split,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQ_LEN\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av2-nTMwVg3i"
      },
      "source": [
        "# create the vocabulary indexing with `adapt`\n",
        "vectorization_layer.adapt(X_train_raw.batch(BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha0e8zdNZM9A",
        "outputId": "e1422f91-7b65-4537-9c8f-72e082582fc6"
      },
      "source": [
        "print(f\"Total unique characters: {len(vectorization_layer.get_vocabulary())}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique characters: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfACnSAHcLo7",
        "outputId": "e606410c-9f70-44a0-e618-e40cf6d3fc90"
      },
      "source": [
        "print(vectorization_layer.get_vocabulary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', ' ', 'e', 't', 'a', 'o', 'n', 'h', 'i', 's', 'r', 'd', 'l', 'w', 'u', 'f', 'g', 'm', 'y', 'b', 'c', 'p', 'k', 'v', 'j', 'q', 'x', 'z', 'ó', 'É', 'ú', 'û', 'é', 'á', 'í', 'ë', 'â', 'ä', 'Ó']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlqZ_iM2cQGT"
      },
      "source": [
        "def vectorize_text(text):\n",
        "    \"\"\" Convert text into a Tensor using vectorization_layer\"\"\"\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return tf.squeeze(vectorization_layer(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNWzpoeyc9q9",
        "outputId": "24674c2c-ef3c-4566-9b1b-658d6d5d7da6"
      },
      "source": [
        "test_text = \"hello i am Hoaha\"\n",
        "\n",
        "vectorize_text(test_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(20,), dtype=int64, numpy=\n",
              "array([ 8,  3, 13, 13,  6,  2,  9,  2,  5, 18,  2,  8,  6,  5,  8,  5,  0,\n",
              "        0,  0,  0])>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbCiQhJpd5F-"
      },
      "source": [
        "#### Apply Text Vectorization to X & y datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yG2zJ-LdE8U",
        "outputId": "86a50c67-2810-4f86-c468-d343dd224c28"
      },
      "source": [
        "# vectorize the dataset\n",
        "X_train = X_train_raw.map(vectorize_text)\n",
        "y_train = y_train_raw.map(vectorize_text)\n",
        "\n",
        "X_train.element_spec, y_train.element_spec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(20,), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(20,), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9Pj4qNKedwa",
        "outputId": "83d92ee7-0f57-408c-e319-92ee1224e315"
      },
      "source": [
        "for elem in y_train.take(10):\n",
        "    print(elem)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(20,), dtype=int64)\n",
            "tf.Tensor([24  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0], shape=(20,), dtype=int64)\n",
            "tf.Tensor([7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(20,), dtype=int64)\n",
            "tf.Tensor([23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0], shape=(20,), dtype=int64)\n",
            "tf.Tensor([7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(20,), dtype=int64)\n",
            "tf.Tensor([10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0], shape=(20,), dtype=int64)\n",
            "tf.Tensor([15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0], shape=(20,), dtype=int64)\n",
            "tf.Tensor([12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0], shape=(20,), dtype=int64)\n",
            "tf.Tensor([11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0], shape=(20,), dtype=int64)\n",
            "tf.Tensor([4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(20,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnWusGJVeuhK"
      },
      "source": [
        "# we only one the first representation in the vector in the y_train dataset\n",
        "y_train = y_train.map(lambda y: y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJEa6PQvfbEU",
        "outputId": "47fcb6ed-ee19-42b6-c87b-c5350d207105"
      },
      "source": [
        "for elem in y_train.take(5):\n",
        "    print(f\"Shape: {elem.shape}\")\n",
        "    print(f\"Next Character: {elem.numpy()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: ()\n",
            "Next Character: 3\n",
            "Shape: ()\n",
            "Next Character: 24\n",
            "Shape: ()\n",
            "Next Character: 7\n",
            "Shape: ()\n",
            "Next Character: 23\n",
            "Shape: ()\n",
            "Next Character: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBFi_L2lgL2R",
        "outputId": "f995db00-6073-484e-ce8a-f9b8f667c745"
      },
      "source": [
        "# Check tensor dimensions to ensure we have MAX_SEQ_LEN-sized inputs and single output\n",
        "X_train.take(1), y_train.take(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<TakeDataset shapes: (20,), types: tf.int64>,\n",
              " <TakeDataset shapes: (), types: tf.int64>)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q7FdsXOg2XQ",
        "outputId": "24ea6572-7ff8-42f6-ee8b-6aca79ccd764"
      },
      "source": [
        "for input, output in zip(X_train.take(5), y_train.take(5)):\n",
        "    print(f\"{input.numpy()} ------------>  {output.numpy()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 4  8 11  3  3  2 11  9  7 17 10  2 16  6 11  2  4  8  3  2] ------------>  3\n",
            "[11  3  3  2 11  9  7 17 10  2 16  6 11  2  4  8  3  2  3 13] ------------>  24\n",
            "[ 3  2 11  9  7 17 10  2 16  6 11  2  4  8  3  2  3 13 24  3] ------------>  7\n",
            "[11  9  7 17 10  2 16  6 11  2  4  8  3  2  3 13 24  3  7  0] ------------>  23\n",
            "[ 7 17 10  2 16  6 11  2  4  8  3  2  3 13 24  3  7 23  9  0] ------------>  7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8p3YiJRg_hP"
      },
      "source": [
        "#### Bringing the data pipeline together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRbr9Ub1hJ8K"
      },
      "source": [
        "**Joining the X and y into a dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6G8kWIphMw-"
      },
      "source": [
        "# joining X & y into a single dataset\n",
        "train_dataset = tf.data.Dataset.zip((X_train, y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N8Q5VeghVSa"
      },
      "source": [
        "**Setting data pipeline optimizations:**\n",
        "Perform async prefetching / buffering of data using AUTOTUNE\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY_1f_H0hfXH"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_dataset = train_dataset.prefetch(buffer_size=512).batch(BATCH_SIZE, drop_remainder=True).cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06t7wVLEhvBY",
        "outputId": "28b7317f-ecca-4ea2-975d-32c59f4abbf5"
      },
      "source": [
        "print(f\"Size of the dataset in batches: {train_dataset.cardinality().numpy()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the dataset in batches: 11974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3LTZqwKh3Eg",
        "outputId": "a94c1d64-6027-4bb5-fd5f-8100f0bad166"
      },
      "source": [
        "# check the tensor dimensions of X and y again\n",
        "\n",
        "for sample in train_dataset.take(1):\n",
        "    print(f\"Input (X) Dimension: {sample[0].numpy().shape}\")\n",
        "    print(f\"Output (y) Dimension: {sample[1].numpy().shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input (X) Dimension: (64, 20)\n",
            "Output (y) Dimension: (64,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCDje6OdiOBv"
      },
      "source": [
        "### Build the LSTM Model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff-C3pnnlR1D"
      },
      "source": [
        "def char_LSTM_model(max_seq_len=MAX_SEQ_LEN, max_features=MAX_FEATURES, embedding_dim=EMBEDDING_DIM):\n",
        "\n",
        "    # Define input for the model (vocab indices)\n",
        "    inputs = tf.keras.Input(shape=(max_seq_len), dtype=\"int64\")\n",
        "\n",
        "    # Add a layer to map the vocab indices into an embedding layer\n",
        "    X = Embedding(max_features, embedding_dim)(inputs)\n",
        "    X = Dropout(0.5)(X)\n",
        "    X = LSTM(128, return_sequences=True)(X)\n",
        "    X = Flatten()(X)\n",
        "    outputs = Dense(max_features, activation=\"softmax\")(X)\n",
        "    model = Model(inputs, outputs, name=\"model_LSTM\")\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpI324yXrCCy",
        "outputId": "8ceaac43-e15e-4211-9efa-4ba6955078b9"
      },
      "source": [
        "model = char_LSTM_model()\n",
        "optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_LSTM\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20)]              0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 20, 16)            1488      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 20, 16)            0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 20, 128)           74240     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2560)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 93)                238173    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 313,901\n",
            "Trainable params: 313,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ing-AtSxllnH"
      },
      "source": [
        "def sample(preds, temperature=0.2):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds=np.squeeze(preds)\n",
        "    \n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "    \n",
        "def generate_text(model, seed_original, step, diversity):\n",
        "    seed=vectorize_text(seed_original)\n",
        "    # decode_sentence(seed.numpy().squeeze())\n",
        "    print(f\"Starting the sentence with.... '{seed_original}'\")\n",
        "    print(\"...Diversity:\", diversity)\n",
        "    seed= vectorize_text(seed_original).numpy().reshape(1,-1)\n",
        "    \n",
        "    generated = (seed)\n",
        "    for i in range(step):\n",
        "        predictions=model.predict(seed)\n",
        "        pred_max= np.argmax(predictions.squeeze())\n",
        "        next_index = sample(predictions, diversity)\n",
        "        generated = np.append(generated, next_index)\n",
        "        seed= generated[-MAX_SEQ_LEN:].reshape(1,MAX_SEQ_LEN)\n",
        "    return decode_sentence(generated)\n",
        "\n",
        "\n",
        "def decode_sentence (encoded_sentence):\n",
        "    deceoded_sentence=[]\n",
        "    for word in encoded_sentence:\n",
        "        deceoded_sentence.append(vectorization_layer.get_vocabulary()[word])\n",
        "    sentence= ''.join(deceoded_sentence)\n",
        "    print(sentence)\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSj2zhTQrTZe"
      },
      "source": [
        "# Create a callback that saves the model's weights\n",
        "checkpoint_path = r\"/content/drive/MyDrive/Colab Notebooks/predictive_keyboard/models/model_cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
        "                                                 save_weights_only=True, \n",
        "                                                 verbose=1)\n",
        "\n",
        "# Train the model\n",
        "epochs = 30\n",
        "BATCH_SIZE = 64\n",
        "SAMPLING_STEPS = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"-\"*40 + f\"  Epoch: {epoch}/{epochs}  \" + \"-\"*40)\n",
        "    model.fit(train_dataset, batch_size=BATCH_SIZE, epochs=1, callbacks=[cp_callback])\n",
        "    print()\n",
        "    print(\"*\"*30 + f\" Generating text after epoch #{epoch} \" + \"*\"*30)\n",
        "    start_index = random.randint(0, len(text) - MAX_SEQ_LEN - 1)\n",
        "    sentence = text[start_index : start_index + MAX_SEQ_LEN]\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        generate_text(model, sentence, SAMPLING_STEPS, diversity)\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXRI53ovotoS"
      },
      "source": [
        "model.save(r\"/content/drive/MyDrive/Colab Notebooks/predictive_keyboard/models/Char_LSTM_LOTR_20211112-1.h5\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mqnuiBxclrR"
      },
      "source": [
        "### Learning Rate Finder\n",
        "We will use Leslie Smith's Learning Rate finder technique to plot the change of the loss function of the model when the learning rate is exponentially increasing\n",
        "References: \n",
        "- https://www.kaggle.com/paultimothymooney/learning-rate-finder-for-keras\n",
        "- https://www.pyimagesearch.com/2019/08/05/keras-learning-rate-finder/\n",
        "- https://www.fast.ai/\n",
        "- https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6APUyEIf5MF"
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.callbacks import LambdaCallback\n",
        "import math\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUtsfVSKiF7y",
        "outputId": "384b4f2c-f0df-469a-afe1-9bb44ea15855"
      },
      "source": [
        "list(train_dataset.batch(1).as_numpy_iterator())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11974"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZecwOEneQR0"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "class LRFinder:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.losses = []\n",
        "        self.lrs = []\n",
        "        self.best_loss = 1e9\n",
        "\n",
        "    def on_batch_end(self, batch, logs):\n",
        "        # obtain learning rate for the batch\n",
        "        lr = K.get_value(self.model.optimizer.lr)\n",
        "        self.lrs.append(lr)\n",
        "\n",
        "        # log the loss\n",
        "        loss = logs['loss']\n",
        "        self.losses.append(loss)\n",
        "\n",
        "        # check whether the loss is too huge or NaN\n",
        "        if math.isnan(loss) or loss> self.best_loss * 4:\n",
        "            self.model.stop_training = True\n",
        "            return\n",
        "\n",
        "        # take the winner\n",
        "        if loss < self.best_loss: \n",
        "            self.best_loss = loss\n",
        "    \n",
        "        # increase learning rate for the next batch\n",
        "        lr *= self.lr_mult\n",
        "\n",
        "    def find(self, train_dataset, start_lr, end_lr, batch_size=BATCH_SIZE, epochs=1):\n",
        "        for elem in train_dataset.take(1).as_numpy_iterator(): rows = elem[0].shape[0]\n",
        "        num_batches = epochs * rows / batch_size\n",
        "        self.lr_mult = (end_lr / start_lr) ** (1 / num_batches)\n",
        "\n",
        "        # save weights to a file\n",
        "        self.model.save_weights(\"tmp.h5\")\n",
        "\n",
        "        # remember the original learning rate\n",
        "        original_lr = K.get_value(self.model.optimizer.lr)\n",
        "\n",
        "        # set the initial learning rate\n",
        "        K.set_value(self.model.optimizer.lr, start_lr)\n",
        "\n",
        "        callback = LambdaCallback(on_batch_end = lambda batch, logs: self.on_batch_end(batch, logs))\n",
        "\n",
        "        self.model.fit(train_dataset, batch_size=batch_size, epochs=epochs, callbacks=[callback])\n",
        "\n",
        "        # restore the weights to the state before model fitting\n",
        "        self.model.load_weights('tmp.h5')\n",
        "\n",
        "        # restore the original learning rate\n",
        "        K.set_value(self.model.optimizer.lr, original_lr)\n",
        "    \n",
        "    def plot_loss(self, n_skip_beginning=10, n_skip_end=5):\n",
        "        \"\"\"\n",
        "        Plots the loss\n",
        "        Args:\n",
        "            n_skip_beginning = number of batches to skip on the left\n",
        "            n_skip_end = number of batches to skip on the right.\n",
        "        \"\"\"\n",
        "        plt.ylabel(\"loss\")\n",
        "        plt.xlabel(\"Learning Rate (log)\")\n",
        "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], \n",
        "                 self.losses[n_skip_beginning:-n_skip_end])\n",
        "        plt.xscale(\"log\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, \n",
        "                         y_lim=(-0.01, 0.01)):\n",
        "        \"\"\"\n",
        "        Plots rate of change to the loss function.\n",
        "        Args:\n",
        "            sma = number of batches for simple moving average (curve smoothing)\n",
        "            n_skip_beginning = number of batches to skip on the left\n",
        "            n_skip_end = number of batches to skip on the right.\n",
        "            y_lim = limits for the y-axis\n",
        "        \"\"\"\n",
        "\n",
        "        assert sma >= 1\n",
        "        derivatives = [0] * sma\n",
        "        for i in range(sma, len(self.lrs)):\n",
        "            derivative = (self.losses[i] - self.losses[i - sma] / sma)\n",
        "            derivatives.append(derivative)\n",
        "        \n",
        "        plt.ylabel(\"Rate of Loss Change\")\n",
        "        plt.xlabel(\"Learning Rate (log)\")\n",
        "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end],\n",
        "                 derivatives[n_skip_beginning:-n_skip_end])\n",
        "        plt.xscale(\"log\")\n",
        "        plt.ylim(y_lim)\n",
        "        plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "YM6ANj-cmL-t",
        "outputId": "d064a932-c928-4bdc-ab3e-d5e275ea22e3"
      },
      "source": [
        "# initialize lr finder object\n",
        "lr_finder = LRFinder(model)\n",
        "\n",
        "# find lr\n",
        "lr_finder.find(train_dataset, start_lr = 0.0001, end_lr = 1, batch_size=BATCH_SIZE, epochs=5)\n",
        "\n",
        "# plot loss\n",
        "lr_finder.plot_loss()\n",
        "\n",
        "# plot rate of loss change\n",
        "lr_finder.plot_loss_change()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "11974/11974 [==============================] - 150s 13ms/step - loss: 1.4348 - accuracy: 0.5519\n",
            "Epoch 2/5\n",
            "11974/11974 [==============================] - 151s 13ms/step - loss: 1.4215 - accuracy: 0.5553\n",
            "Epoch 3/5\n",
            "11974/11974 [==============================] - 150s 13ms/step - loss: 1.4182 - accuracy: 0.5565\n",
            "Epoch 4/5\n",
            "11974/11974 [==============================] - 150s 13ms/step - loss: 1.4148 - accuracy: 0.5573\n",
            "Epoch 5/5\n",
            "11974/11974 [==============================] - 150s 13ms/step - loss: 1.4139 - accuracy: 0.5576\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV3qVQZfsZDa"
      },
      "source": [
        "Oops didn't seem to work. Let's try one-hot encoding for the input vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utmzz69cqNJ6"
      },
      "source": [
        "## One-Hot Encoding Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vms7T5NFWU5u"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc9e8WgEWXvi"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras.layers import Embedding, Input, LSTM, Flatten, Dense, Dropout\n",
        "from tensorflow.keras import Model\n",
        "import numpy as np \n",
        "\n",
        "from pprint import pprint as pp\n",
        "from string import punctuation\n",
        "import regex as re\n",
        "import random\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZBNcAXU4Wqi"
      },
      "source": [
        "#### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eKUs3AB4Wqi"
      },
      "source": [
        "path = r\"/content/drive/MyDrive/Colab Notebooks/predictive_keyboard/data/lotr_full.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_QPWJZG4Wqi",
        "outputId": "90e0605f-33d8-456e-c5fb-ac70b617d286"
      },
      "source": [
        "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "print(text[:1000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Three Rings for the Elven-kings under the sky,\n",
            "               Seven for the Dwarf-lords in their halls of stone,\n",
            "            Nine for Mortal Men doomed to die,\n",
            "              One for the Dark Lord on his dark throne\n",
            "           In the Land of Mordor where the Shadows lie.\n",
            "               One Ring to rule them all, One Ring to find them,\n",
            "               One Ring to bring them all and in the darkness bind them\n",
            "           In the Land of Mordor where the Shadows lie.\n",
            "           \n",
            "FOREWORD\n",
            "\n",
            "This tale grew in the telling, until it became a history of the Great War of the Ring and included many glimpses of the yet more ancient history that preceded it. It was begun soon after _The Hobbit_ was written and before its publication in 1937; but I did not go on with this sequel, for I wished first to complete and set in order the mythology and legends of the Elder Days, which had then been taking shape for some years. I desired to do this for my own satisfaction, and I had little hope that other people \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnfkGhRo4Wqj",
        "outputId": "c72b630e-1582-4674-ff53-847e0fbf3a8e"
      },
      "source": [
        "print(f\"Corpus length: {int(len(text)) / 1000 } K characters\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus length: 1532.723 K characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6KDl30qtgNe"
      },
      "source": [
        "def standardize_text_string(text: str):\n",
        "    \"\"\"\n",
        "        create a custom standardization that:\n",
        "            1. Fixes whitespaces \n",
        "            2. Removes punctuations & numbers\n",
        "            3. Sets all texts to lowercase\n",
        "            4. Preserves the Elvish characters\n",
        "    \"\"\"\n",
        "    \n",
        "    text = re.sub(r\"[\\s+]\", \" \", text)\n",
        "    text = re.sub(r\"[0-9]\", \"\", text)\n",
        "    text = re.sub(f\"[{punctuation}–]\", \"\", text)\n",
        "\n",
        "    return text.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEqgIvEtssNP"
      },
      "source": [
        "# get unique characters in the text\n",
        "chars = sorted(set(standardize_text_string(text)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQpwILNTtaXn",
        "outputId": "eba929f5-8562-461d-e604-10c2ddd005cb"
      },
      "source": [
        "print(chars, f\"\\n\\nTotal unique characters: {len(chars)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'á', 'â', 'ä', 'é', 'ë', 'í', 'ó', 'ú', 'û'] \n",
            "\n",
            "Total unique characters: 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N5boivwtbF4"
      },
      "source": [
        "# create dictionary mappings for chars to integers for vectorization & vice versa\n",
        "char2int = {c: i for i, c in enumerate(chars)}\n",
        "int2char = {i: c for c, i in char2int.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppDB2eqWujEi",
        "outputId": "4b808deb-a354-4a7d-cd28-f2c36dc1e018"
      },
      "source": [
        "print(char2int)\n",
        "print(int2char)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, 'á': 27, 'â': 28, 'ä': 29, 'é': 30, 'ë': 31, 'í': 32, 'ó': 33, 'ú': 34, 'û': 35}\n",
            "{0: ' ', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 27: 'á', 28: 'â', 29: 'ä', 30: 'é', 31: 'ë', 32: 'í', 33: 'ó', 34: 'ú', 35: 'û'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT7z5iWW35B2"
      },
      "source": [
        "standardized_text = standardize_text_string(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zklBFovrshdU",
        "outputId": "9c32a728-5887-458e-b2db-8c175a0dc894"
      },
      "source": [
        "# preserve the same sequence length and step\n",
        "MAX_SEQ_LEN = 20\n",
        "step = 2\n",
        "EMBEDDING_DIM = 16\n",
        "\n",
        "# let's collect our sentences and next_chars again\n",
        "\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0, len(standardized_text) - MAX_SEQ_LEN, step):\n",
        "    sentences.append(standardized_text[i: i + MAX_SEQ_LEN])\n",
        "    next_chars.append(standardized_text[i + MAX_SEQ_LEN])\n",
        "\n",
        "print(\"Total number of training examples:\", len(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of training examples: 736848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auq1fW8ZulLl"
      },
      "source": [
        "# get the total number of unique chars\n",
        "N_UNIQUE_CHARS = len(chars)\n",
        "m = len(sentences)\n",
        "MAX_SEQ_LEN = MAX_SEQ_LEN # 20; (see above)\n",
        "\n",
        "def vectorize_sentence(text, max_seq_len=MAX_SEQ_LEN, n_unique_chars=N_UNIQUE_CHARS):\n",
        "    \"\"\" Convert input sentence into numpy vector of shape (m, max_seq_len, n_unique_chars) \"\"\"\n",
        "    if type(text) == str:\n",
        "        # if text is input as string\n",
        "        if len(text) > max_seq_len:\n",
        "            # if text is longer than max_seq_len it will be truncated \n",
        "            ## and appended on the list\n",
        "            text_list = []\n",
        "            for i in range(0, len(text), max_seq_len):\n",
        "                text_list.append(text[i: i+max_seq_len])\n",
        "            text = text_list\n",
        "        else:\n",
        "            # if text is less than max_seq_len, convert str -> list(str)\n",
        "            text = [text]\n",
        "        \n",
        "    \n",
        "    m = len(text) # get total number of sentences\n",
        "\n",
        "    x = np.zeros((m, max_seq_len, n_unique_chars), dtype=np.bool)\n",
        "    for i, sentence in enumerate(text):\n",
        "        # for each sentence in the `text` list\n",
        "        for p, char in enumerate(sentence.lower()): \n",
        "            # p is the position of the letter in the sentence\n",
        "            # char is the character in the sentence\n",
        "            x[i, p, char2int[char]] = 1\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmGPT_egvTr7",
        "outputId": "263a984b-fcd2-419f-c7bf-f1de5f9dc8fd"
      },
      "source": [
        "# try out sentence to ensure we get \n",
        "text_test = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" + \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\".lower()\n",
        "\n",
        "text_test_vector = vectorize_sentence(text_test)\n",
        "print(\"Shape of text vector: {}\".format(text_test_vector.shape))\n",
        "print(f\"Supposed shape:{(round(len(text_test) / MAX_SEQ_LEN), MAX_SEQ_LEN, N_UNIQUE_CHARS)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of text vector: (3, 20, 36)\n",
            "Supposed shape:(3, 20, 36)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgCNwUaA2TP5"
      },
      "source": [
        "Nice! We got the right shape. Let's vectorize our `sentence` using the `vectorize_sentence` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4iRZLJL2Fs2",
        "outputId": "dbf359d8-fa66-4cf1-80bf-6cb583b79d03"
      },
      "source": [
        "# vectorize input sentences\n",
        "X_data = vectorize_sentence(sentences);\n",
        "print(X_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(736848, 20, 36)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ436LRL4qPc"
      },
      "source": [
        "> Supposed shape: `(len(sentences), MAX_SEQ_LEN, N_UNIQUE_CHARS)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tHNxwSv2OfG",
        "outputId": "97b6525f-5cad-4238-e1e3-48c1ab563e9b"
      },
      "source": [
        "# vectorize next_chars (output) -> shape: (m, N_UNIQUE_CHARS)\n",
        "y_data = np.zeros((m, N_UNIQUE_CHARS), dtype=np.bool)\n",
        "for i, char in enumerate(next_chars):\n",
        "    y_data[i, char2int[char]] = 1\n",
        "\n",
        "print(y_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(736848, 36)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE7AfpwEW2b4"
      },
      "source": [
        "def char_LSTM_model(max_seq_len=MAX_SEQ_LEN, max_features=N_UNIQUE_CHARS, embedding_dim=EMBEDDING_DIM):\n",
        "\n",
        "    # Define input for the model (vocab indices)\n",
        "    inputs = tf.keras.Input(shape=(max_seq_len, max_features))\n",
        "\n",
        "    # No embedding for one-hot encoding\n",
        "    # X = Embedding((max_seq_len, max_features), (max_features, embedding_dim))(inputs)\n",
        "\n",
        "    X = LSTM(128, return_sequences=True)(inputs)\n",
        "    X = Flatten()(X)\n",
        "    outputs = Dense(max_features, activation=\"softmax\")(X)\n",
        "    model = Model(inputs, outputs, name=\"model_LSTM\")\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VglUcNoXeqO",
        "outputId": "a29627bd-8958-442f-e947-4d717f48acbd"
      },
      "source": [
        "model_onehot = char_LSTM_model()\n",
        "optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model_onehot.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "model_onehot.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_LSTM\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_11 (InputLayer)       [(None, 20, 36)]          0         \n",
            "                                                                 \n",
            " lstm_8 (LSTM)               (None, 20, 128)           84480     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2560)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 36)                92196     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 176,676\n",
            "Trainable params: 176,676\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeI1aOEbX2Vw"
      },
      "source": [
        "def sample(preds, temperature=0.2):\n",
        "    # helper function to sample an index from a probability array  \n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "    \n",
        "def generate_text(model, seed_original, step, diversity):\n",
        "    seed=vectorize_sentence(seed_original) # shape-> (1,20,36)\n",
        "    # decode_sentence(seed.numpy().squeeze())\n",
        "    print(f\"Starting the sentence with.... '{seed_original}'\")\n",
        "    print(\"...Diversity:\", diversity)\n",
        "    # seed= vectorize_sentence(seed_original).reshape(1,-1)\n",
        "    \n",
        "    generated = (seed)\n",
        "    for i in range(step):\n",
        "        predictions=model.predict(seed)[0]\n",
        "        next_index = sample(predictions, diversity)\n",
        "        next_char = int2char[next_index]\n",
        "        generated = np.append(generated, next_index)\n",
        "\n",
        "    return decode_sentence(generated)\n",
        "\n",
        "\n",
        "def decode_sentence (encoded_sentence):\n",
        "    deceoded_sentence=[]\n",
        "    for word in encoded_sentence:\n",
        "        deceoded_sentence.append(int2char[word])\n",
        "    sentence= ''.join(deceoded_sentence)\n",
        "    print(sentence)\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "JspPhZ0UK_Jc",
        "outputId": "4708c090-78e3-4c79-cdb4-a53a80bdd590"
      },
      "source": [
        "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "    generate_text(model_onehot, sentence, SAMPLING_STEPS, diversity)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting the sentence with.... 'as sleeping down at '\n",
            "...Diversity: 0.2\n",
            " a                                                     a                a                                                      a                            a                            a                                   a                                              a                            a                                        a                            a                            a                                       a                                              a                                           a                          a                     a                                    a                                                      a               a                                   tttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt\n",
            "\n",
            "Starting the sentence with.... 'as sleeping down at '\n",
            "...Diversity: 0.5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-9ed68a39ef58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdiversity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAMPLING_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-108-4b57bb3e1d02>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, seed_original, step, diversity)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint2char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1783\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1785\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1786\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 726\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    749\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 751\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3236\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3237\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 3238\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   3239\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3240\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "NSkWhRAM5xQa",
        "outputId": "b0d92100-1792-44a5-e2a9-fbf5562c43f3"
      },
      "source": [
        "# Create a callback that saves the model's weights\n",
        "checkpoint_path = r\"/content/drive/MyDrive/Colab Notebooks/predictive_keyboard/models/one_hot/model_cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
        "                                                 save_weights_only=True, \n",
        "                                                 verbose=1)\n",
        "\n",
        "# Train the model\n",
        "epochs = 30\n",
        "BATCH_SIZE = 64\n",
        "SAMPLING_STEPS = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"-\"*40 + f\"  Epoch: {epoch}/{epochs}  \" + \"-\"*40)\n",
        "    model_onehot.fit(X_data, y_data, batch_size=BATCH_SIZE, epochs=1, callbacks=[cp_callback])\n",
        "    print()\n",
        "    print(\"*\"*30 + f\" Generating text after epoch #{epoch} \" + \"*\"*30)\n",
        "    start_index = random.randint(0, len(text) - MAX_SEQ_LEN - 1)\n",
        "    sentence = standardized_text[start_index : start_index + MAX_SEQ_LEN]\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        generate_text(model_onehot, sentence, SAMPLING_STEPS, diversity)\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------  Epoch: 0/30  ----------------------------------------\n",
            "11513/11514 [============================>.] - ETA: 0s - loss: 1.7822 - accuracy: 0.4664\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/Colab Notebooks/predictive_keyboard/models/one_hot/model_cp.ckpt\n",
            "11514/11514 [==============================] - 114s 9ms/step - loss: 1.7822 - accuracy: 0.4664\n",
            "\n",
            "****************************** Generating text after epoch #0 ******************************\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-6708ed11baf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardized_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdiversity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAMPLING_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-21a178606280>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, seed_original, step, diversity)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_original\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# decode_sentence(seed.numpy().squeeze())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting the sentence with.... '{seed_original}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vectorize_text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq68WrcQYDyh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}